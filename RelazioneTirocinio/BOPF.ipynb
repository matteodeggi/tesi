{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saxpy.sax import sax_via_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il metodo sax_via_window della libreria saxpy restituisce un dict con key = parola e value = posizione/i della parola\n",
    "# questo metodo estrae dal dict le parole e le inserisce in un array n volte, con n la lunghezza del value\n",
    "def extract_sax_words(dictionary, filter_words = None):\n",
    "    \n",
    "    words = []\n",
    "    for key, value in dictionary.items():\n",
    "        number = len(value)\n",
    "        if (filter_words):\n",
    "            if (key in dict(filter_words)):\n",
    "                for i in range(0, number):\n",
    "                    words.append(key)\n",
    "        else:\n",
    "            for i in range(0, number):\n",
    "                words.append(key)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasforma le ts in sax representation\n",
    "def sax(valori_ts, subsequence_length, word_length):\n",
    "    \n",
    "    # ts_sax contiene la rappresentazione di ogni time series\n",
    "    ts_sax = []\n",
    "    # words contiene tutte le parole\n",
    "    words = []\n",
    "    valori_ts = np.array(valori_ts)\n",
    "    \n",
    "    for ts in valori_ts:\n",
    "        sax_words = sax_via_window(ts, subsequence_length, word_length, 4)\n",
    "        ts_sax.append(extract_sax_words(sax_words))\n",
    "        words.extend(list(sax_words.keys()))\n",
    "        \n",
    "    return ts_sax, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasforma le features del dataset in parole e lo popola con il conto delle parole\n",
    "def preprocess_data(data, words, ts_sax):\n",
    "    \n",
    "    for word in set(words):\n",
    "        data[word] = 0\n",
    "    data = data.drop(np.arange(9, 454), axis = 1)\n",
    "    for ts, row in zip(ts_sax, data.iterrows()):\n",
    "        counter = Counter(ts)\n",
    "        for word in counter.keys():\n",
    "            data.at[row[0], word] = counter[word]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcola l'anova f-score per ogni feature (parola)\n",
    "def compute_anova(data, words):\n",
    "    anova_values = []\n",
    "    class_features = []\n",
    "\n",
    "    for word in set(words):\n",
    "        class_features = []\n",
    "        for i in range(0, 11):\n",
    "            class_features.append(data[word][data[8] == i])\n",
    "        anova_values.append((word, stats.f_oneway(*class_features)[0]))\n",
    "    \n",
    "    return anova_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distanza di una misurazione dal centroide, word_set è l'insieme delle features da tenere in considerazione\n",
    "def centroid_distance(centroids, data, word_set):\n",
    "    distance = 0.0\n",
    "    \n",
    "    for centroid, word in zip(centroids, word_set):\n",
    "        distance += (data[word] - centroid)**2\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo del term frequency–inverse document frequency\n",
    "def tfidf(word, classe, data):\n",
    "    return tf(word, classe, data)*idf(word, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo del term-frequency\n",
    "def tf(word, classe, data):\n",
    "    ct = sum(data[word][data['classe'] == classe])\n",
    "    if (ct == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1+math.log(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo dell'inverse document frequency\n",
    "def idf(word, data):\n",
    "    c = len(set(data['classe']))\n",
    "    wc = 0\n",
    "    for i in range(0, c):\n",
    "        if (any(data[word][data['classe'] == i] > 0)):\n",
    "            wc += 1\n",
    "    \n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misura di distance di una misurazione dal tf-idf, word_set è l'insieme delle features da tenere in considerazione\n",
    "def squared_cosine_similarity(tfidfs, data, word_set):\n",
    "    num = 0.0\n",
    "    den1 = 0.0\n",
    "    den2 = 0.0\n",
    "    \n",
    "    for tfidf, word in zip(tfidfs, word_set):\n",
    "        num += (data[word]*tfidf)**2\n",
    "        den1 += data[word]\n",
    "        den2 += tfidf\n",
    "    \n",
    "    distance = num/(den1*den2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation per il calcolo dei centroidi per ogni classe\n",
    "def cv_centroids(data, anova, max_features, w, l):\n",
    "    \n",
    "    # trasformo i dati in array, dato che LeaveOneOut non accetta come argomento un DataFrame di Pandas\n",
    "    X = np.array(data.drop(8, axis = 1))\n",
    "    y = np.array(data[8])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    best_accuracy = 0.00\n",
    "    \n",
    "    # eseguo la cross-validation aumentando ad ogni ciclo il numero di feature da prendere in considerazione\n",
    "    for k in range(1, max_features):\n",
    "        \n",
    "        matches = 0\n",
    "        \n",
    "        # cross-validation\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "            y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "            \n",
    "            # ritrasfromo in DataFrame, per facilità di manipolazione\n",
    "            X_train_CV = pd.DataFrame(X_train_CV, columns= [column for column in data.drop(8, axis = 1)])\n",
    "            X_test_CV = pd.DataFrame(X_test_CV, columns= [column for column in data.drop(8, axis = 1)])\n",
    "            X_train_CV['classe'] = y_train_CV\n",
    "            \n",
    "            # prendo le top k features per anova value\n",
    "            topk = nlargest(k, anova, key=itemgetter(1))\n",
    "            words = [word[0] for word in topk]\n",
    "\n",
    "            class_centroids = []\n",
    "            c_distance = float('Inf')\n",
    "            classe = 0\n",
    "            \n",
    "            # calcolo i centroidi per ogni classe\n",
    "            for i in range(0, 11):\n",
    "                centroids = []\n",
    "                for word in set(words):\n",
    "                    ct = sum(X_train_CV[word][X_train_CV['classe'] == i])\n",
    "                    stc = (X_train_CV['classe'] == i).value_counts()[1]\n",
    "                    centroids.append(ct/stc)\n",
    "                class_centroids.append(centroids)\n",
    "                # distanza di X_test dai centroidi calcolati\n",
    "                distance = centroid_distance(centroids, X_test_CV, words)[0]\n",
    "                # se la distanza è minore della distanza più piccola incontrata in precedenza, a X_test viene assegnata la classe i\n",
    "                if (distance < c_distance):\n",
    "                    c_distance = distance\n",
    "                    classe = i\n",
    "            if (classe == y_test_CV):\n",
    "                matches += 1\n",
    "        \n",
    "        # calcolo accuracy \n",
    "        accuracy = matches/(loo.get_n_splits(X))*100\n",
    "        if (accuracy > best_accuracy):\n",
    "            best_k = k\n",
    "            best_accuracy = accuracy\n",
    "            best_features = words\n",
    "            best_centroids = class_centroids\n",
    "        \n",
    "        tipo = 'C'\n",
    "    \n",
    "    return best_k, best_accuracy, best_features, best_centroids, w, l, tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation per il calcolo dei tf-idf per ogni classe\n",
    "def cv_tfidf(data, anova, max_features, w, l):\n",
    "    X = np.array(data.drop(8, axis = 1))\n",
    "    y = np.array(data[8])\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    best_accuracy = 0.00\n",
    "\n",
    "    for k in range(1, max_features):\n",
    "        \n",
    "        matches = 0\n",
    "        \n",
    "        for train_index, test_index in loo.split(X):\n",
    "            X_train_CV, X_test_CV = X[train_index], X[test_index]\n",
    "            y_train_CV, y_test_CV = y[train_index], y[test_index]\n",
    "            X_train_CV = pd.DataFrame(X_train_CV, columns= [column for column in data.drop(8, axis = 1)])\n",
    "            X_test_CV = pd.DataFrame(X_test_CV, columns= [column for column in data.drop(8, axis = 1)])\n",
    "            X_train_CV['classe'] = y_train_CV\n",
    "\n",
    "            topk = nlargest(k, anova, key=itemgetter(1))\n",
    "            words = [word[0] for word in topk]\n",
    "\n",
    "            class_tfidf = []\n",
    "            c_distance = float('Inf')\n",
    "            classe = 0\n",
    "\n",
    "            for i in range(0, 11):\n",
    "                tfidfs = []\n",
    "                for word in set(words):\n",
    "                    tfidfs.append(tfidf(word, i, X_train_CV))\n",
    "                class_tfidf.append(tfidfs)\n",
    "                distance = squared_cosine_similarity(tfidfs, X_test_CV, words)[0]\n",
    "                if (distance < c_distance):\n",
    "                    c_distance = distance\n",
    "                    classe = i\n",
    "            if (classe == y_test_CV):\n",
    "                matches += 1\n",
    "\n",
    "        accuracy = matches/(loo.get_n_splits(X))*100\n",
    "        if (accuracy > best_accuracy):\n",
    "            best_k = k\n",
    "            best_accuracy = accuracy\n",
    "            best_features = words\n",
    "            best_tfidf = class_tfidf\n",
    "            \n",
    "    tipo = 'T'\n",
    "    \n",
    "    return best_k, best_accuracy, best_features, best_tfidf, w, l, tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estraiamo dai selettori restituitici dalle cross-validation i più performanti\n",
    "def selection(co1, co2):\n",
    "    co = []\n",
    "\n",
    "    top2_c = nlargest(2, co1, key=itemgetter(1))\n",
    "    top2_t = nlargest(2, co2, key=itemgetter(1))\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "\n",
    "    for co1, co2 in zip(top2_c, top2_t):\n",
    "        sum1 += co1[1]\n",
    "        sum2 += co2[1]\n",
    "\n",
    "    aaco1 = sum1/2\n",
    "    aaco2 = sum2/2\n",
    "    if (aaco1 > 0.7*max(aaco1, aaco2)):\n",
    "        co.extend(top2_c)\n",
    "    \n",
    "    if (aaco2 > 0.7*max(aaco1, aaco2)):\n",
    "        co.extend(top2_t)\n",
    "        \n",
    "    return co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estraiamo da un selettore le features, i centroidi/tf-idf, la word-length, la window-length, il tipo (centroide o tf-idf)\n",
    "def extract_co(co):\n",
    "    F = co[2]\n",
    "    t = co[3]\n",
    "    w = co[4]\n",
    "    l = co[5]\n",
    "    tipo = co[6]\n",
    "    \n",
    "    return F, t, w, l, tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione che unisce i due step di rappresentazione sax e trasformazione del dataset\n",
    "def BOP(ts, w, l):\n",
    "    ts_sax, words = sax(ts, l, w)\n",
    "    data = preprocess_data(ts, words, ts_sax)\n",
    "    \n",
    "    return data, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifica un dataset\n",
    "def classify(ts_bop, features, T, tipo):\n",
    "    Hco = []\n",
    "    \n",
    "    for ts in np.array(ts_bop):\n",
    "        ts = pd.DataFrame(ts.reshape(1,-1), columns= [column for column in ts_bop])\n",
    "        c_distance = float('Inf')\n",
    "        \n",
    "        # calcolo delle distanze dai centroidi/tf-idf di ogni classe\n",
    "        for i in range(0, 11):\n",
    "            if (tipo == 'T'):\n",
    "                distance = squared_cosine_similarity(T[i], ts, features)[0]\n",
    "            else:\n",
    "                distance = centroid_distance(T[i], ts, features)[0]\n",
    "                \n",
    "            if (distance < c_distance):\n",
    "                c_distance = distance\n",
    "                classe = i\n",
    "                \n",
    "        Hco.append(classe)\n",
    "    return Hco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicazione del voto di maggioranza alla classificazione di più selettori\n",
    "def majority_vote(H_co):\n",
    "\n",
    "    predictions = list(zip(*H_co))\n",
    "    majority_vote = []\n",
    "    \n",
    "    for i in range(0, len(H_co[0])):\n",
    "        counter = Counter(predictions[i])\n",
    "        majority_vote.append(counter.most_common(1)[0][0])\n",
    "    \n",
    "    return majority_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOPF_fit(X_train, y_train):\n",
    "    # array per storare le combinazioni restituite dalle cross-validation\n",
    "    c_combinations = []\n",
    "    t_combinations = []\n",
    "\n",
    "    # array di word-length e window-length da testare\n",
    "    word_length = [4, 5, 6]\n",
    "    window_length = np.arange(0.04*X_train.shape[0], 0.2*X_train.shape[0], 0.04*X_train.shape[0]).astype(int)\n",
    "\n",
    "    for w in word_length:\n",
    "        for l in window_length:\n",
    "            print('Word Length: ', w)\n",
    "            print('Window Length: ', l)\n",
    "            print('Sax Approximation...')\n",
    "            data, words = BOP(X_train, w, l)\n",
    "            print('Computing ANOVA values...')\n",
    "            anova_values = compute_anova(data, words)\n",
    "            print('Cross-validating centroids...')\n",
    "            c_combinations.append(cv_centroids(data, anova_values, 5, w, l))\n",
    "            print('Cross-validating tfidf...')\n",
    "            t_combinations.append(cv_tfidf(data, anova_values, 5, w, l))\n",
    "    \n",
    "    print('Best predictors selection...')\n",
    "    co = selection(c_combinations, t_combinations)\n",
    "    return co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOPF_predict(co, X_test):\n",
    "    H_co = []\n",
    "    # per ogni combinazione/selettore, classifico il dataset di test\n",
    "    for c in co:\n",
    "        F, T, w, l, tipo = extract_co(c)\n",
    "        T_BOP, words = BOP(X_test, w, l)\n",
    "        H_co.append(classify(T_BOP, F, T, tipo))\n",
    "    \n",
    "    y_pred = majority_vote(H_co)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Length:  4\n",
      "Window Length:  11\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  4\n",
      "Window Length:  28\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  4\n",
      "Window Length:  46\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  4\n",
      "Window Length:  64\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  5\n",
      "Window Length:  11\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  5\n",
      "Window Length:  28\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  5\n",
      "Window Length:  46\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  5\n",
      "Window Length:  64\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  6\n",
      "Window Length:  11\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  6\n",
      "Window Length:  28\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  6\n",
      "Window Length:  46\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n",
      "Word Length:  6\n",
      "Window Length:  64\n",
      "Sax Approximation...\n",
      "Computing ANOVA values...\n",
      "Cross-validating centroids...\n",
      "Cross-validating tfidf...\n"
     ]
    }
   ],
   "source": [
    "# leggo il dataset\n",
    "\n",
    "data = pd.read_csv('Swissex.meta.csv', header = None, sep = ',')\n",
    "data = data.drop(np.arange(0,8), axis=1)\n",
    "\n",
    "# split del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(8, axis = 1), data[8], test_size = 0.3, random_state = 100)\n",
    "X_train[8] = y_train\n",
    "\n",
    "# array per storare le combinazioni restituite dalle cross-validation\n",
    "c_combinations = []\n",
    "t_combinations = []\n",
    "\n",
    "# array di word-length e window-length da testare\n",
    "word_length = [4, 5, 6]\n",
    "window_length = np.arange(0.025*len(data.shape[0]), 0.18*len(data.shape[0]), 0.04*len(data.shape[0])).astype(int)\n",
    "\n",
    "for w in word_length:\n",
    "    for l in window_length:\n",
    "        print('Word Length: ', w)\n",
    "        print('Window Length: ', l)\n",
    "        print('Sax Approximation...')\n",
    "        data, words = BOP(X_train, w, l)\n",
    "        print('Computing ANOVA values...')\n",
    "        anova_values = compute_anova(data, words)\n",
    "        print('Cross-validating centroids...')\n",
    "        c_combinations.append(cv_centroids(data, anova_values, 5, w, l))\n",
    "        print('Cross-validating tfidf...')\n",
    "        t_combinations.append(cv_tfidf(data, anova_values, 5, w, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictors selection...\n"
     ]
    }
   ],
   "source": [
    "print('Best predictors selection...')\n",
    "co = selection(c_combinations, t_combinations)\n",
    "H_co = []\n",
    "# per ogni combinazione/selettore, classifico il dataset di test\n",
    "for c in co:\n",
    "    F, T, w, l, tipo = extract_co(c)\n",
    "    T_BOP, words = BOP(X_test, w, l)\n",
    "    H_co.append(classify(T_BOP, F, T, tipo))\n",
    "\n",
    "y_pred = majority_vote(H_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.76923076923077"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.068295163988946"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_pred, y_test, average = 'macro')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo distanza damerau-levenshtein\n",
    "\n",
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    d = {}\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    for i in range(-1,lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in range(-1,lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    "\n",
    "    for i in range(lenstr1):\n",
    "        for j in range(lenstr2):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1, # deletion\n",
    "                           d[(i,j-1)] + 1, # insertion\n",
    "                           d[(i-1,j-1)] + cost, # substitution\n",
    "                          )\n",
    "            if i and j and s1[i]==s2[j-1] and s1[i-1] == s2[j]:\n",
    "                d[(i,j)] = min (d[(i,j)], d[i-2,j-2] + cost) # transposition\n",
    "\n",
    "    return d[lenstr1-1,lenstr2-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Classifier(BaseEstimator):\n",
    "    \n",
    "    MAX_DIST = 1000\n",
    "    dictionary = {}\n",
    "    probs = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.classes = set(y_train)\n",
    "        for c in self.classes:\n",
    "            self.dictionary[c] = []\n",
    "        \n",
    "        X_train = X_train['metadata']\n",
    "        for i, word in enumerate(X_train):\n",
    "            c1 = y_train[i]\n",
    "            self.dictionary[c1].append(word)\n",
    "            \n",
    "    def predict_proba(self, X_test):\n",
    "        \n",
    "        self.probs = []\n",
    "        for i, word in enumerate(X_test['metadata']):\n",
    "            \n",
    "            edit_distances = []\n",
    "            for c1 in self.classes:\n",
    "                dam = [damerau_levenshtein_distance(word, field) for field in self.dictionary[c1]]\n",
    "                if len(dam) > 0:\n",
    "                    # aggiungiamo 1 perch√® divideremo dopo\n",
    "                    edit_distances.append((min(dam)+1))\n",
    "                else:\n",
    "                    edit_distances.append(MAX_DIST)\n",
    "            \n",
    "            self.probs.append(np.true_divide(1, edit_distances))\n",
    "            \n",
    "        return self.probs   \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        if (len(self.probs) == 0):\n",
    "            self.probs = self.predict_proba(X_test)\n",
    "        y_pred = []\n",
    "        for series in self.probs:\n",
    "            y_pred.append(np.argmax(series))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {'metadata': []}\n",
    "y = []\n",
    "\n",
    "path = \"\"\n",
    "\n",
    "# calcoliamo le features di ogni timeseries\n",
    "\n",
    "with open(path + 'ThingspeakEU.meta.csv', 'r', encoding='utf-8') as dati:\n",
    "    for row in dati:\n",
    "        riga = row.strip().split(',')\n",
    "        classe = int(riga[8])\n",
    "        y.append(classe)\n",
    "        valore = riga[1]\n",
    "        X['metadata'].append(valore)\n",
    "        \n",
    "X = pd.DataFrame(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = nlp.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6891679748822606"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
